name: observability
services:
  # -------------------------------------------------------
  # 1. Metrics Storage (Prometheus)
  # -------------------------------------------------------
  prometheus:
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          memory: 512M
    image: prom/prometheus:v3.9.0
    container_name: infra-prometheus
    restart: unless-stopped
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle" # 설정 리로드 허용
      - "--web.enable-remote-write-receiver"
    volumes:
      - ./prometheus/config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/config/alert_rules:/etc/prometheus/alert_rules
      - prometheus-data:/prometheus
    # ports:
    #   - "${PROMETHEUS_HOST_PORT}:${PROMETHEUS_PORT}"
    networks:
      infra_net:
        ipv4_address: 172.19.0.30
    healthcheck:
      test: ["CMD", "wget", "-q", "--tries=1", "--spider", "http://localhost:${PROMETHEUS_PORT}/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    labels:
      - "traefik.enable=true"
      # 도메인: prometheus.${DEFAULT_URL}
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${DEFAULT_URL}`)"
      - "traefik.http.routers.prometheus.entrypoints=websecure"
      - "traefik.http.routers.prometheus.tls=true"
      - "traefik.http.services.prometheus.loadbalancer.server.port=${PROMETHEUS_PORT}"
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
  # -------------------------------------------------------
  # 2. Log Storage (Loki)
  # -------------------------------------------------------
  loki:
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
    image: grafana/loki:3.6.3
    container_name: infra-loki
    command:
      - "-config.file=/etc/loki/loki-config.yaml"
      - "-config.expand-env=true"
    networks:
      infra_net:
        ipv4_address: 172.19.0.31
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --tries=1 --spider http://localhost:${LOKI_PORT}/ready || exit 1"]
      interval: 10s
      retries: 5
      start_period: 60s
    ports:
      - "${LOKI_HOST_PORT}:${LOKI_PORT}"
    volumes:
      - ./loki/config/loki-config.yaml:/etc/loki/loki-config.yaml:ro
      - loki-data:/loki
    environment:
      - MINIO_APP_USERNAME=${MINIO_APP_USERNAME}
      - MINIO_APP_USER_PASSWORD=${MINIO_APP_USER_PASSWORD}
    restart: unless-stopped
  # -------------------------------------------------------
  # 3. Trace Storage (Tempo)
  # -------------------------------------------------------
  tempo:
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          memory: 512M
    image: grafana/tempo:2.9.0
    container_name: infra-tempo
    command:
      - "-config.file=/etc/tempo.yaml"
      - "-config.expand-env=true"
    volumes:
      - ./tempo/config/tempo.yaml:/etc/tempo.yaml:ro
      - tempo-data:/var/tempo
    networks:
      infra_net:
        ipv4_address: 172.19.0.32
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    ports:
      - "${TEMPO_HOST_PORT}:${TEMPO_PORT}"
    environment:
      - MINIO_APP_USERNAME=${MINIO_APP_USERNAME}
      - MINIO_APP_USER_PASSWORD=${MINIO_APP_USER_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:${TEMPO_PORT}/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
  # -------------------------------------------------------
  # 4. Visualization (Grafana)
  # -------------------------------------------------------
  grafana:
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    image: grafana/grafana:12.3.1
    container_name: infra-grafana
    environment:
      - GF_SERVER_ROOT_URL=https://grafana.${DEFAULT_URL}
      - GF_SERVER_DOMAIN=grafana.${DEFAULT_URL}

      # OAuth 자동 로그인 활성화
      # true로 설정하면, 접속 시 로그인 화면을 건너뛰고 바로 Keycloak 인증을 시도합니다.
      - GF_AUTH_OAUTH_AUTO_LOGIN=true

      # Grafana 기본 로그인 폼 숨기기
      # ID/PW 입력창을 없애고 오직 OAuth(Keycloak)만 사용하게 강제합니다.
      - GF_AUTH_DISABLE_LOGIN_FORM=true

      # [2] 보안 및 기본 관리자 계정 (초기 세팅용)
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USERNAME}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false # 일반 회원가입 비활성화 (OAuth로만 가입)
      # (선택) 자동 생성된 유저의 기본 권한 (Viewer, Editor, Admin)
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Editor
      - GF_LOG_LEVEL=debug

      # [3] Keycloak OAuth2 연동 (Generic OAuth)
      - GF_AUTH_GENERIC_OAUTH_ENABLED=true
      - GF_AUTH_GENERIC_OAUTH_NAME=Keycloak
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true # Keycloak 인증 성공 시 Grafana 유저 자동 생성
      - GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH=preferred_username
      - GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH=name

      # Keycloak 클라이언트 정보 (.env 파일에 변수 정의 필요)
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=${OAUTH2_PROXY_CLIENT_ID} # 예: nginx-client
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${OAUTH2_PROXY_CLIENT_SECRET}

      # Scopes: 'groups'가 반드시 포함되어야 함 (Client Scope 설정 필요)
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email offline_access groups

      # [해결책] 내부 통신(Token Exchange) 시 사설 인증서 에러 무시
      - GF_AUTH_GENERIC_OAUTH_TLS_SKIP_VERIFY_INSECURE=true

      # Keycloak Endpoints (Realm 이름이 'myrealm'인 경우의 예시, 변수화 추천)
      # ${KEYCLOAK_REALM} 부분은 본인의 Realm 이름으로 변경하세요.
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://keycloak.${DEFAULT_URL}/realms/hy-home.realm/protocol/openid-connect/auth
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=https://keycloak.${DEFAULT_URL}/realms/hy-home.realm/protocol/openid-connect/token
      - GF_AUTH_GENERIC_OAUTH_API_URL=https://keycloak.${DEFAULT_URL}/realms/hy-home.realm/protocol/openid-connect/userinfo
      # 로그아웃 리다이렉트 (필수)
      # Grafana에서 로그아웃하면 Keycloak 세션도 같이 끊어주거나, 다시 로그인 화면으로 보내야 합니다.
      # 아래 URL은 Keycloak의 로그아웃 엔드포인트입니다.
      # post_logout_redirect_uri는 로그아웃 후 다시 돌아올 주소입니다.
      - GF_AUTH_SIGNOUT_REDIRECT_URL=https://keycloak.${DEFAULT_URL}/realms/hy-home.realm/protocol/openid-connect/logout?post_logout_redirect_uri=https://grafana.${DEFAULT_URL}/login

      # 권한 매핑 (Role Mapping) - 핵심
      # Keycloak의 'groups' 정보를 읽어서 Grafana 권한(Admin/Editor/Viewer)으로 변환
      # 로직: groups 배열에 '/admins'이 있으면 'Admin', 아니면 'Viewer' 부여
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(groups[*], '/admins') && 'Admin' || contains(groups[*], '/editors') && 'Editor' || 'Viewer'

      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_STRICT=true
      # (옵션) '/admin' 그룹 유저에게 Grafana 전체 서버 관리자(Super Admin) 권한까지 부여하려면 true
      - GF_AUTH_GENERIC_OAUTH_ALLOW_ASSIGN_GRAFANA_ADMIN=true
      - GF_AUTH_GENERIC_OAUTH_GRAFANA_ADMIN_ATTRIBUTE_PATH=contains(groups[*], '/admins')
      # [5] 기타 기능 설정
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
      # [추가] PKCE 활성화: Keycloak의 보안 요구 사항을 충족시킵니다.
      - GF_AUTH_GENERIC_OAUTH_USE_PKCE=true
      # S256 방식을 명시적으로 지정 (필요 시)
      - GF_AUTH_GENERIC_OAUTH_CODE_CHALLENGE_METHOD=S256
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/etc/grafana/dashboards
      - grafana-data:/var/lib/grafana
    networks:
      infra_net:
        ipv4_address: 172.19.0.33
    # ports:
    #   - "${GRAFANA_HOST_PORT}:${GRAFANA_PORT}"
    healthcheck:
      # Grafana 내부의 전용 상태 확인 API (Application Programming Interface) 사용
      test: ["CMD-SHELL", "wget -q --spider http://localhost:${GRAFANA_PORT}/api/health || exit 1"]
      interval: 10s # 상태 확인 주기
      timeout: 5s # 응답 대기 시간
      retries: 3 # 실패 시 재시도 횟수
      start_period: 30s # 초기 JVM 기동 및 DB 연결 대기 시간
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_started
      tempo:
        condition: service_started
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      # 도메인: grafana.${DEFAULT_URL}
      - "traefik.http.routers.grafana.rule=Host(`grafana.${DEFAULT_URL}`)"
      - "traefik.http.routers.grafana.entrypoints=websecure"
      - "traefik.http.routers.grafana.tls=true"
      - "traefik.http.services.grafana.loadbalancer.server.port=${GRAFANA_PORT}"
      - "traefik.http.routers.grafana.middlewares=sso-auth@file"
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
  # -------------------------------------------------------
  # 5. Telemetry Collector (Alloy)
  # -------------------------------------------------------
  alloy:
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    image: grafana/alloy:v1.12.1
    container_name: infra-alloy
    restart: unless-stopped
    user: "0:0"
    command: ["run", "--server.http.listen-addr=0.0.0.0:${ALLOY_PORT}", "/etc/alloy/config.alloy"]
    volumes:
      - ./alloy/config/config.alloy:/etc/alloy/config.alloy:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro # 도커 로그 접근
      - /var/run/docker.sock:/var/run/docker.sock:ro # 도커 메타데이터 접근
    ports:
      #   - "${ALLOY_HOST_PORT}:${ALLOY_PORT}"
      - "${ALLOY_OTLP_GRPC_HOST_PORT}:${ALLOY_OTLP_GRPC_PORT}" # OTLP gRPC Receiver
      - "${ALLOY_OTLP_HTTP_HOST_PORT}:${ALLOY_OTLP_HTTP_PORT}" # OTLP HTTP Receiver
    networks:
      infra_net:
        ipv4_address: 172.19.0.34
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_started
      tempo:
        condition: service_started
    labels:
      - "traefik.enable=true"
      # Alloy UI: alloy.${DEFAULT_URL}
      - "traefik.http.routers.alloy.rule=Host(`alloy.${DEFAULT_URL}`)"
      - "traefik.http.routers.alloy.entrypoints=websecure"
      - "traefik.http.routers.alloy.tls=true"
      - "traefik.http.services.alloy.loadbalancer.server.port=${ALLOY_PORT}"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:${ALLOY_PORT}/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
  # -------------------------------------------------------
  # 6. Container Metrics (cAdvisor)
  # -------------------------------------------------------
  cadvisor:
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    image: gcr.io/cadvisor/cadvisor:v0.55.1
    container_name: cadvisor
    hostname: cadvisor
    restart: unless-stopped
    privileged: true
    # 윈도우(WSL2)에서는 kmsg 접근이 불안정할 수 있습니다.
    # 만약 실행이 안 되면 아래 devices 섹션을 주석 처리하세요.
    devices:
      - /dev/kmsg
    expose:
      - ${CADVISOR_PORT}
    volumes:
      # 호스트(VM)의 루트 파일시스템
      - /:/rootfs:ro
      # Docker 소켓 및 런타임 정보 (필수)
      - /var/run:/var/run:ro
      # 시스템 정보 (cgroup 등, 필수)
      - /sys:/sys:ro
      # 도커 데이터 디렉터리
      - /var/lib/docker/:/var/lib/docker:ro
      # 윈도우에서는 /dev/disk 매핑이 에러를 유발하므로 제거하는 것이 좋습니다.
      # - /dev/disk/:/dev/disk:ro

      # [추가] machine-id 에러 완화를 위한 시도
      # 윈도우 Docker Desktop VM에 해당 파일이 없을 수도 있지만, 시도해볼 가치는 있습니다.
      - /etc/machine-id:/etc/machine-id:ro
      - /var/lib/dbus/machine-id:/var/lib/dbus/machine-id:ro
    networks:
      infra_net:
        ipv4_address: 172.19.0.35
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:${CADVISOR_PORT}/healthz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
  alertmanager:
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    image: prom/alertmanager:v0.30.0
    container_name: infra-alertmanager
    environment:
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SLACK_ALERTMANAGER_WEBHOOK_URL=${SLACK_ALERTMANAGER_WEBHOOK_URL}
    entrypoint: ["/bin/sh", "-c"]
    command: |
      if [ -z "$SLACK_ALERTMANAGER_WEBHOOK_URL" ]; then
        echo "SLACK_ALERTMANAGER_WEBHOOK_URL is not set"; exit 1;
      fi
      sed "s|__SLACK_WEBHOOK_URL__|$SLACK_ALERTMANAGER_WEBHOOK_URL|g" /etc/alertmanager/config.yml.template > /etc/alertmanager/config.yml
      exec /bin/alertmanager --config.file=/etc/alertmanager/config.yml --storage.path=/alertmanager
    volumes:
      - ./alertmanager/config/config.yml:/etc/alertmanager/config.yml.template:ro
      - alertmanager-data:/alertmanager
    depends_on:
      prometheus:
        condition: service_healthy
      grafana:
        condition: service_healthy
    networks:
      infra_net:
        ipv4_address: 172.19.0.36
    # ports:
    #   - "${ALERTMANAGER_HOST_PORT}:${ALERTMANAGER_PORT}"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:${ALERTMANAGER_PORT}/-/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    labels:
      - "traefik.enable=true"
      # Alertmanager UI: alertmanager.${DEFAULT_URL}
      - "traefik.http.routers.alertmanager.rule=Host(`alertmanager.${DEFAULT_URL}`)"
      - "traefik.http.routers.alertmanager.entrypoints=websecure"
      - "traefik.http.routers.alertmanager.tls=true"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=${ALERTMANAGER_PORT}"
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
  pushgateway:
    deploy:
      resources:
        limits:
          cpus: "0.2"
          memory: 128M
    image: prom/pushgateway:v1.11.2
    container_name: pushgateway
    restart: unless-stopped
    expose:
      - ${PUSHGATEWAY_PORT}
    # ports:
    #   - "${PUSHGATEWAY_HOST_PORT}:${PUSHGATEWAY_PORT}"
    depends_on:
      prometheus:
        condition: service_healthy
      grafana:
        condition: service_healthy
    networks:
      infra_net:
        ipv4_address: 172.19.0.37
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:${PUSHGATEWAY_PORT}/-/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    labels:
      - "traefik.enable=true"
      # pushgateway UI: pushgateway.${DEFAULT_URL}
      - "traefik.http.routers.pushgateway.rule=Host(`pushgateway.${DEFAULT_URL}`)"
      - "traefik.http.routers.pushgateway.entrypoints=websecure"
      - "traefik.http.routers.pushgateway.tls=true"
      - "traefik.http.services.pushgateway.loadbalancer.server.port=${PUSHGATEWAY_PORT}"
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

volumes:
  tempo-data:
    # driver: local
    # driver_opts:
    #   o: bind
    #   type: none
    #   device: ${DEFAULT_OBSERVABILITY_DIR}/tempo
  loki-data:
    # driver: local
    # driver_opts:
    #   o: bind
    #   type: none
    #   device: ${DEFAULT_OBSERVABILITY_DIR}/loki
  alertmanager-data:
    # driver: local
    # driver_opts:
    #   o: bind
    #   type: none
    #   device: ${DEFAULT_OBSERVABILITY_DIR}/alertmanager
  prometheus-data:
    # driver: local
    # driver_opts:
    #   o: bind
    #   type: none
    #   device: ${DEFAULT_OBSERVABILITY_DIR}/prometheus
  grafana-data:
    # driver: local
    # driver_opts:
    #   o: bind
    #   type: none
    #   device: ${DEFAULT_OBSERVABILITY_DIR}/grafana

networks:
  infra_net:
    name: infra_net
    driver: bridge
    ipam:
      config:
        - subnet: ${INFRA_SUBNET:-172.19.0.0/16}
          gateway: ${INFRA_GATEWAY:-172.19.0.1}
