groups:
  # ============================================================================
  # [0] Prometheus Self-Monitoring (Standard Mixins)
  # ============================================================================
  - name: prometheus_alerts
    rules:
      # [What] Prometheus Job Missing / 프로메테우스 잡 누락
      # [Why] A target job defined in configuration is not present. / 설정된 타겟 잡이 존재하지 않습니다.
      - alert: PrometheusJobMissing
        expr: absent(up{job="prometheus"})
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Prometheus job missing (instance {{ $labels.instance }})
          description:
            "프로메테우스 잡이 사라졌습니다.\n  VALUE = {{ $value }}\n  LABELS =
            {{ $labels }}"

      # [What] Prometheus Target Missing / 프로메테우스 타겟 다운
      # [Why] A specific target instance is down. / 특정 타겟 인스턴스가 다운되었습니다.
      - alert: PrometheusTargetMissing
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Prometheus target missing (instance {{ $labels.instance }})
          description:
            "프로메테우스 타겟이 사라졌습니다. 익스포터가 크래시되었을 수
            있습니다.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # [What] All Targets Missing / 모든 타겟 다운
      # [Why] All targets for a critical job are down. / 특정 잡의 모든 타겟이 응답하지 않습니다.
      - alert: PrometheusAllTargetsMissing
        expr: sum by (job) (up) == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Prometheus all targets missing (job {{ $labels.job }})
          description:
            "프로메테우스 잡에 활성 타겟이 없습니다.\n  VALUE = {{ $value
            }}\n  LABELS = {{ $labels }}"

      # [What] Configuration Reload Failure / 설정 리로드 실패
      # [Why] Prometheus failed to reload its configuration. / 프로메테우스 설정 리로드에 실패했습니다.
      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful != 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary:
            Prometheus configuration reload failure (instance {{
            $labels.instance }})
          description:
            "프로메테우스 설정 리로드에 실패했습니다.\n  VALUE = {{ $value
            }}\n  LABELS = {{ $labels }}"

      # [What] Alertmanager Connection Failed / 얼럿매니저 연결 실패
      # [Why] Prometheus cannot send alerts to Alertmanager. / 프로메테우스가 얼럿매니저에 연결할 수 없습니다.
      - alert: PrometheusNotConnectedToAlertmanager
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary:
            Prometheus not connected to alertmanager (instance {{
            $labels.instance }})
          description:
            "프로메테우스가 얼럿매니저에 연결할 수 없습니다.\n  VALUE = {{
            $value }}\n  LABELS = {{ $labels }}"

      # [What] Prometheus Too Many Restarts / 프로메테우스 과다 재시작
      # [Why] Prometheus might be crashlooping. / 프로메테우스가 크래시루프 상태일 수 있습니다.
      - alert: PrometheusTooManyRestarts
        expr:
          changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m])
          > 2
        for: 0m
        labels:
          severity: warning
        annotations:
          summary:
            "Prometheus too many restarts (instance {{ $labels.instance }})"
          description:
            "프로메테우스가 15분 내 2회 이상 재시작했습니다.\n  VALUE = {{
            $value }}\n  LABELS = {{ $labels }}"

      # [What] Prometheus Rule Evaluation Failures / 규칙 평가 실패
      # [Why] Rules are failing to evaluate. / 규칙 평가에 실패하고 있습니다.
      - alert: PrometheusRuleEvaluationFailures
        expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary:
            "Prometheus rule evaluation failures (instance {{ $labels.instance
            }})"
          description:
            "프로메테우스 규칙 평가에 실패했습니다.\n  VALUE = {{ $value
            }}\n  LABELS = {{ $labels }}"

      # [What] Prometheus TSDB Compaction Failed / TSDB 압축 실패
      # [Why] TSDB compaction failure may cause storage issues. / TSDB 압축 실패로 스토리지 문제가 발생할 수 있습니다.
      - alert: PrometheusTsdbCompactionsFailed
        expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary:
            "Prometheus TSDB compactions failed (instance {{ $labels.instance
            }})"
          description:
            "TSDB 압축에 실패했습니다.\n  VALUE = {{ $value }}\n  LABELS = {{
            $labels }}"

      # [What] Alertmanager Config Not Synced / 얼럿매니저 설정 미동기화
      # [Why] Config sync issues between Alertmanager instances. / 얼럿매니저 인스턴스 간 설정 동기화 문제.
      - alert: PrometheusAlertmanagerConfigNotSynced
        expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Alertmanager config not synced"
          description:
            "얼럿매니저 인스턴스 간 설정이 동기화되지 않았습니다.\n  VALUE = {{
            $value }}\n  LABELS = {{ $labels }}"

  # ============================================================================
  # [1] Infrastructure (Container & cAdvisor)
  # ============================================================================
  - name: infrastructure_alerts
    rules:
      # [What] High CPU Usage / CPU 사용률 높음
      # [Why] Container CPU usage is consistently high (>80%). / 컨테이너 CPU 사용량이 5분 이상 80%를 초과했습니다.
      - alert: ContainerHighCpuUtilization
        expr:
          (sum(rate(container_cpu_usage_seconds_total{name!=""}[5m])) by (name)
          /
          sum(container_spec_cpu_quota{name!=""}/container_spec_cpu_period{name!=""})
          by (name) * 100) > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "CPU 과부하 지속: {{ $labels.name }}"
          description:
            "{{ $labels.name }} 컨테이너가 CPU 80% 이상을 점유
            중입니다.\n  VALUE = {{ $value }}"

      # [What] High Memory Usage / 메모리 사용률 높음
      # [Why] Container working set memory is above 80% of limit. / 워킹셋 메모리가 할당량의 80%를 초과했습니다.
      - alert: ContainerHighMemoryUsage
        expr:
          (sum(container_memory_working_set_bytes{name!=""}) BY (instance, name)
          / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) *
          100) > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "메모리 사용량 높음: {{ $labels.name }}"
          description:
            "{{ $labels.name }} 컨테이너가 메모리 리밋의 80% 이상을 사용
            중입니다.\n  VALUE = {{ $value }}"

      # [What] Container Memory Critical / 메모리 위험 수준
      # [Why] Container memory usage covers >90% of its limit. / 컨테이너 메모리 사용량이 할당량의 90%를 초과했습니다.
      - alert: ContainerMemoryCritical
        expr:
          (sum(container_memory_working_set_bytes{name!=""}) BY (instance, name)
          / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) *
          100) > 90
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "메모리 부족 임박: {{ $labels.name }}"
          description:
            "{{ $labels.name }} 컨테이너가 메모리 리밋의 90% 이상을 사용
            중입니다. OOM 위험이 있습니다.\n  VALUE = {{ $value }}"

      # [What] Container Killed / 컨테이너 종료 감지
      # [Why] A container disappeared suddenly (possible crash/OOM). / 컨테이너가 갑자기 사라졌습니다 (크래시 또는 OOM 가능성).
      - alert: ContainerKilled
        expr: time() - container_last_seen > 60
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "컨테이너 종료 감지: {{ $labels.name }}"
          description:
            "{{ $labels.name }} ({{ $labels.instance }}) 컨테이너가 60초 이상
            보이지 않습니다."

      # [What] Container Absent / 컨테이너 부재
      # [Why] Expected container has disappeared for 5 minutes. / 기대되는 컨테이너가 5분간 사라졌습니다.
      - alert: ContainerAbsent
        expr: absent(container_last_seen)
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "컨테이너 부재"
          description: "기대되는 컨테이너가 5분 이상 보이지 않습니다."

      # [What] Container Volume Usage High / 볼륨 사용량 높음
      # [Why] Container volume usage is above 80%. / 컨테이너 볼륨 사용량이 80%를 초과했습니다.
      - alert: ContainerVolumeUsage
        expr:
          (1 - (sum(container_fs_inodes_free{name!=""}) BY (instance) /
          sum(container_fs_inodes_total) BY (instance))) * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "컨테이너 볼륨 사용량 높음: {{ $labels.instance }}"
          description:
            "컨테이너 볼륨 사용량이 80%를 초과했습니다.\n  VALUE = {{ $value }}"

      # [What] Container CPU Throttled / CPU 쓰로틀링
      # [Why] Container is being throttled due to CPU limits. / CPU 제한으로 컨테이너가 쓰로틀링되고 있습니다.
      - alert: ContainerHighThrottleRate
        expr:
          sum(increase(container_cpu_cfs_throttled_periods_total{container!=""}[5m]))
          by (container, pod, namespace) /
          sum(increase(container_cpu_cfs_periods_total[5m])) by (container, pod,
          namespace) > 0.25
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "컨테이너 CPU 쓰로틀링: {{ $labels.container }}"
          description:
            "컨테이너가 25% 이상 CPU 쓰로틀링되고 있습니다.\n  VALUE = {{ $value
            }}"

  # ============================================================================
  # [2] n8n Automation
  # ============================================================================
  - name: n8n_alerts
    rules:
      # [What] Workflow Failed / 워크플로우 실패
      # [Why] Critical business logic failed. / 중요한 비즈니스 로직 실행에 실패했습니다.
      - alert: N8nWorkflowFailed
        expr:
          increase(n8n_workflow_execution_count_total{status="error"}[5m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "n8n 워크플로우 실패 감지"
          description:
            "workflow_id={{ $labels.workflow_id }} 실행 중 에러가 발생했습니다."

      # [What] Leader Instance Missing / 리더 인스턴스 누락
      # [Why] High availability mechanism is broken. / 고가용성(HA) 메커니즘이 깨졌습니다.
      - alert: N8nLeaderMissing
        expr: n8n_instance_role_leader != 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "n8n 리더 인스턴스 이상"
          description:
            "n8n 인스턴스 {{ $labels.instance }} 가 리더가 아니거나 리더
            플래그가 내려갔습니다."

  # ============================================================================
  # [3] Databases (Redis)
  # ============================================================================
  - name: redis_alerts
    rules:
      # [What] Redis Down / Redis 다운
      # [Why] Service is unavailable. / 서비스 접속이 불가능합니다.
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis 다운: {{ $labels.instance }}"
          description:
            "Redis 인스턴스 {{ $labels.instance }} 가 1분 이상 down 상태입니다."

      # [What] Redis Memory Critical / Redis 메모리 위험
      # [Why] Redis is running out of memory. / Redis 메모리가 고갈되고 있습니다.
      - alert: RedisMemoryCritical
        expr:
          redis_memory_max_bytes > 0 and on (instance) (redis_memory_used_bytes
          / redis_memory_max_bytes > 0.9)
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis 메모리 90% 초과: {{ $labels.instance }}"
          description:
            "Redis 인스턴스 {{ $labels.instance }} 메모리 사용률이 90%를
            초과했습니다."

      # [What] Redis Rejected Connections / Redis 연결 거부
      # [Why] Max clients limit reached. / 최대 클라이언트 접속 한도에 도달했습니다.
      - alert: RedisRejectedConnections
        expr: increase(redis_rejected_connections_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis 연결 거부 발생: {{ $labels.instance }}"
          description:
            "Redis가 연결을 거부하고 있습니다. Max Clients 설정을 확인하세요."

      # [What] Redis Missing Master / Redis 마스터 없음
      # [Why] Redis cluster has no node marked as master. / Redis 클러스터에 마스터로 표시된 노드가 없습니다.
      - alert: RedisMissingMaster
        expr: (count(redis_instance_info{role="master"}) or vector(0)) < 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis 마스터 노드 없음"
          description: "Redis 클러스터에 마스터 노드가 없습니다."

      # [What] Redis Too Many Masters / Redis 마스터 과다
      # [Why] Redis cluster has multiple nodes marked as master. / Redis 클러스터에 여러 마스터가 있습니다.
      - alert: RedisTooManyMasters
        expr: count(redis_instance_info{role="master"}) > 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis 마스터 노드 과다"
          description: "Redis 클러스터에 마스터 노드가 2개 이상입니다."

      # [What] Redis Replication Broken / Redis 복제 중단
      # [Why] Redis instance lost a slave. / Redis 인스턴스에서 슬레이브가 사라졌습니다.
      - alert: RedisReplicationBroken
        expr: delta(redis_connected_slaves[1m]) < 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Redis 복제 중단"
          description: "Redis 인스턴스에서 슬레이브가 사라졌습니다."

      # [What] Redis Cluster Flapping / Redis 클러스터 불안정
      # [Why] Replica connections are frequently changing. / 복제 연결이 자주 변경되고 있습니다.
      - alert: RedisClusterFlapping
        expr: changes(redis_connected_slaves[1m]) > 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Redis 클러스터 불안정"
          description: "Redis 복제 연결이 자주 변경되고 있습니다 (flapping)."

  # ============================================================================
  # [4] Databases (Valkey)
  # ============================================================================
  - name: valkey_alerts
    rules:
      # [What] Valkey Down / Valkey 다운
      # [Why] Service is unavailable. / 서비스 접속이 불가능합니다.
      - alert: ValkeyDown
        expr: valkey_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Valkey 다운: {{ $labels.instance }}"
          description:
            "Valkey 인스턴스 {{ $labels.instance }} 가 1분 이상 down 상태입니다."

      # [What] Valkey Memory Critical / Valkey 메모리 위험
      # [Why] Valkey is running out of memory. / Valkey 메모리가 고갈되고 있습니다.
      - alert: ValkeyMemoryCritical
        expr:
          valkey_memory_max_bytes > 0 and (valkey_memory_used_bytes /
          valkey_memory_max_bytes > 0.9)
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Valkey 메모리 90% 초과: {{ $labels.instance }}"
          description: "Valkey 인스턴스 메모리 사용률이 90%를 초과했습니다."

      # [What] Valkey Rejected Connections / Valkey 연결 거부
      # [Why] Max clients limit reached. / 최대 클라이언트 접속 한도에 도달했습니다.
      - alert: ValkeyRejectedConnections
        expr: increase(valkey_rejected_connections_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Valkey 연결 거부 발생: {{ $labels.instance }}"
          description: "Valkey가 연결을 거부하고 있습니다."

  # ============================================================================
  # [5] Messaging (Kafka)
  # ============================================================================
  - name: kafka_alerts
    rules:
      # [What] Kafka Broker Down / Kafka 브로커 다운
      # [Why] Cluster integrity is compromised. / 클러스터 무결성이 훼손되었습니다.
      - alert: KafkaBrokerDown
        expr: kafka_brokers < 3
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Kafka 브로커 다운 감지"
          description:
            "Kafka 브로커 수가 기대값보다 적습니다. 현재 {{ $value }} 개입니다."

      # [What] Under Replicated Partitions / 복제 미달 파티션
      # [Why] Data redundancy is lost. / 데이터 중복성이 손실되었습니다.
      - alert: KafkaTopicUnderReplicatedPartition
        expr:
          sum(kafka_topic_partition_under_replicated_partition) by (topic) > 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kafka 복제 미달 파티션: {{ $labels.topic }}"
          description: "토픽 {{ $labels.topic }} 의 복제본 수가 부족합니다."

      # [What] Kafka Topics In-Sync Replicas Low / Kafka ISR 부족
      # [Why] Topic partition in-sync replicas are below threshold. / 토픽 파티션 ISR이 임계값 미만입니다.
      - alert: KafkaTopicsReplicas
        expr: min(kafka_topic_partition_in_sync_replica) by (topic) < 2
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Kafka ISR 부족: {{ $labels.topic }}"
          description: "토픽 {{ $labels.topic }}의 ISR 수가 부족합니다."

      # [What] Kafka Consumer Group Lag / Kafka 컨슈머 그룹 지연
      # [Why] Consumer group is lagging behind producer. / 컨슈머 그룹이 프로듀서보다 지연되고 있습니다.
      - alert: KafkaConsumersGroupLag
        expr: sum(kafka_consumergroup_lag) by (consumergroup) > 50
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Kafka 컨슈머 그룹 지연: {{ $labels.consumergroup }}"
          description:
            "컨슈머 그룹 {{ $labels.consumergroup }}의 지연이 50을 초과했습니다."

  # ============================================================================
  # [6] Databases (PostgreSQL)
  # ============================================================================
  - name: postgres_alerts
    rules:
      # [What] Postgres Down / Postgres 다운
      # [Why] Database is unavailable. / 데이터베이스 접속이 불가능합니다.
      - alert: PostgresDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL 다운: {{ $labels.instance }}"
          description:
            "PostgreSQL 인스턴스 {{ $labels.instance }} 가 1분 이상 down
            상태입니다."

      # [What] PostgreSQL Restarted / PostgreSQL 재시작됨
      # [Why] Database was recently restarted. / 데이터베이스가 최근 재시작되었습니다.
      - alert: PostgresqlRestarted
        expr: time() - pg_postmaster_start_time_seconds < 60
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL 재시작됨: {{ $labels.instance }}"
          description: "PostgreSQL이 최근 재시작되었습니다."

      # [What] PostgreSQL Exporter Error / PostgreSQL 익스포터 에러
      # [Why] Exporter is showing errors. / 익스포터가 에러를 보고하고 있습니다.
      - alert: PostgresqlExporterError
        expr: pg_exporter_last_scrape_error > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL 익스포터 에러: {{ $labels.instance }}"
          description: "PostgreSQL 익스포터에서 에러가 발생했습니다."

      # [What] Too Many Connections / 연결 수 과다
      # [Why] Connection pool exhaustion risk. / 커넥션 풀이 고갈될 위험이 있습니다.
      - alert: PostgresTooManyConnections
        expr:
          sum by (instance, job, server) (pg_stat_activity_count) > min by
          (instance, job, server) (pg_settings_max_connections * 0.8)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "DB 연결 수 80% 초과: {{ $labels.instance }}"
          description:
            "PostgreSQL 인스턴스 {{ $labels.instance }} 활성 세션이
            max_connections의 80%를 초과했습니다."

      # [What] High Deadlocks / 교착 상태 증가
      # [Why] Application concurrency issues. / 애플리케이션 동시성 문제가 발생하고 있습니다.
      - alert: PostgresDeadlocks
        expr:
          increase(pg_stat_database_deadlocks{datname!~"template.*|postgres"}[1m])
          > 5
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Postgres 교착 상태 발생: {{ $labels.instance }}"
          description: "최근 1분간 {{ $value }} 개의 데드락이 발생했습니다."

      # [What] High Rollback Rate / 롤백 비율 높음
      # [Why] Transaction failures. / 트랜잭션 실패율이 2%를 초과했습니다.
      - alert: PostgresHighRollbackRate
        expr: >
          sum by (namespace, datname)
          ((rate(pg_stat_database_xact_rollback{datname!~"template.*|postgres",
          datid!="0"}[3m])) /
          ((rate(pg_stat_database_xact_rollback{datname!~"template.*|postgres",
          datid!="0"}[3m])) +
          (rate(pg_stat_database_xact_commit{datname!~"template.*|postgres",
          datid!="0"}[3m])))) > 0.02
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Postgres 롤백 비율 높음"
          description:
            "롤백 비율이 2%를 초과했습니다 (DB: {{ $labels.datname }})."

      # [What] PostgreSQL Replication Lag / PostgreSQL 복제 지연
      # [Why] Replication lag is too high. / 복제 지연이 높습니다.
      - alert: PostgresqlReplicationLag
        expr:
          pg_replication_lag > 30 and ON(instance) pg_replication_is_replica ==
          1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL 복제 지연: {{ $labels.instance }}"
          description: "PostgreSQL 복제 지연이 30초를 초과했습니다."

      # [What] PostgreSQL Table Not Auto Vacuumed / 테이블 자동 VACUUM 미수행
      # [Why] Table has not been auto vacuumed for 10 days. / 테이블이 10일간 자동 VACUUM되지 않았습니다.
      - alert: PostgresqlTableNotAutoVacuumed
        expr:
          ((pg_stat_user_tables_n_tup_del + pg_stat_user_tables_n_tup_upd +
          pg_stat_user_tables_n_tup_hot_upd) >
          pg_settings_autovacuum_vacuum_threshold) and (time() -
          pg_stat_user_tables_last_autovacuum) > 60 * 60 * 24 * 10
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL 테이블 VACUUM 미수행"
          description:
            "테이블 {{ $labels.relname }}이 10일간 자동 VACUUM되지 않았습니다."

  # ============================================================================
  # [7] Search & Object Storage (OpenSearch)
  # ============================================================================
  - name: OpenSearch_alerts
    rules:
      # [What] Cluster Status RED / 클러스터 상태 RED
      # [Why] Data is missing or unavailable. / 데이터가 누락되었거나 사용할 수 없습니다.
      - alert: OpenSearchHealthRed
        expr: elasticsearch_cluster_health_status{color="red"} == 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "OpenSearch 상태 RED"
          description: "OpenSearch 클러스터 상태가 RED 입니다."

      # [What] OpenSearch Cluster Status Yellow / 클러스터 상태 YELLOW
      # [Why] Some replicas are not allocated. / 일부 복제본이 할당되지 않았습니다.
      - alert: OpenSearchClusterYellow
        expr: elasticsearch_cluster_health_status{color="yellow"} == 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "OpenSearch 클러스터 상태 YELLOW"
          description:
            "OpenSearch 클러스터 상태가 YELLOW입니다. 일부 복제본이 할당되지
            않았습니다."

      # [What] High Heap Usage / 힙 메모리 사용 높음
      # [Why] GC overhead or OOM risk. / GC 오버헤드 또는 OOM 위험이 있습니다.
      - alert: OpenSearchHeapUsageTooHigh
        expr:
          (elasticsearch_jvm_memory_used_bytes{area="heap"} /
          elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "OpenSearch 힙 메모리 90% 초과"
          description: "JVM 힙 메모리 사용량이 90%를 넘었습니다."

      # [What] OpenSearch Heap Usage Warning / 힙 메모리 사용 경고
      # [Why] GC overhead risk. / GC 오버헤드 위험이 있습니다.
      - alert: OpenSearchHeapUsageWarning
        expr:
          (elasticsearch_jvm_memory_used_bytes{area="heap"} /
          elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "OpenSearch 힙 메모리 80% 초과"
          description: "JVM 힙 메모리 사용량이 80%를 넘었습니다."

      # [What] OpenSearch Disk Space Low / 디스크 공간 부족
      # [Why] Available disk space is below 20%. / 사용 가능한 디스크 공간이 20% 미만입니다.
      - alert: OpenSearchDiskSpaceLow
        expr:
          elasticsearch_filesystem_data_available_bytes /
          elasticsearch_filesystem_data_size_bytes * 100 < 20
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "OpenSearch 디스크 공간 부족"
          description: "OpenSearch 디스크 사용량이 80%를 초과했습니다."

      # [What] OpenSearch Disk Out of Space / 디스크 공간 없음
      # [Why] Available disk space is below 10%. / 사용 가능한 디스크 공간이 10% 미만입니다.
      - alert: OpenSearchDiskOutOfSpace
        expr:
          elasticsearch_filesystem_data_available_bytes /
          elasticsearch_filesystem_data_size_bytes * 100 < 10
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "OpenSearch 디스크 공간 없음"
          description: "OpenSearch 디스크 사용량이 90%를 초과했습니다."

      # [What] OpenSearch Unassigned Shards / 미할당 샤드
      # [Why] Some shards are not assigned. / 일부 샤드가 할당되지 않았습니다.
      - alert: OpenSearchUnassignedShards
        expr: elasticsearch_cluster_health_unassigned_shards > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "OpenSearch 미할당 샤드"
          description:
            "OpenSearch에서 {{ $value }}개의 샤드가 할당되지 않았습니다."

  # ============================================================================
  # [8] Object Storage (MinIO)
  # ============================================================================
  - name: Minio_alerts
    rules:
      # [What] MinIO Service Down / MinIO 다운
      # [Why] Object storage unavailable. / 오브젝트 스토리지 접속이 불가능합니다.
      - alert: MinioServiceDown
        expr: up{job="minio"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MinIO 서비스 다운: {{ $labels.instance }}"
          description: "MinIO 인스턴스가 응답하지 않습니다."

      # [What] MinIO Cluster Disk Offline / MinIO 클러스터 디스크 오프라인
      # [Why] A disk in the MinIO cluster is offline. / MinIO 클러스터의 디스크가 오프라인입니다.
      - alert: MinioClusterDiskOffline
        expr: minio_cluster_drive_offline_total > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "MinIO 클러스터 디스크 오프라인"
          description: "MinIO 클러스터에서 디스크가 오프라인입니다."

      # [What] MinIO Disk Space Usage / MinIO 디스크 사용량
      # [Why] Available disk space is low. / 사용 가능한 디스크 공간이 부족합니다.
      - alert: MinioDiskSpaceUsage
        expr: disk_storage_available / disk_storage_total * 100 < 10
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "MinIO 디스크 공간 부족"
          description: "MinIO 사용 가능한 디스크 공간이 10% 미만입니다."

  # ============================================================================
  # [9] AI Services (Ollama)
  # ============================================================================
  - name: ollama_alerts
    rules:
      # [What] Ollama Server Down / Ollama 서버 다운
      # [Why] AI inference API unavailable. / AI 추론 API 접속이 불가능합니다.
      - alert: OllamaServerDown
        expr: ollama_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Ollama 서버 다운"
          description: "Ollama HTTP API가 응답하지 않습니다."

  # ============================================================================
  # [10] Gateway & Load Balancers
  # ============================================================================
  - name: gateway_alerts
    rules:
      # [What] HAProxy Down / HAProxy 다운
      # [Why] Traffic entrypoint failure. / 트래픽 진입점 실패.
      - alert: HAProxyDown
        expr: up{job="haproxy"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "HAProxy 다운"
          description: "로드밸런서(HAProxy)가 응답하지 않습니다."

      # [What] Backend Connection Errors / 백엔드 연결 오류
      # [Why] HAProxy cannot reach backend servers. / HAProxy가 백엔드 서버에 연결할 수 없습니다.
      - alert: HAProxyBackendConnectionErrors
        expr:
          sum by (proxy) (rate(haproxy_backend_connection_errors_total[1m])) >
          100
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "HAProxy 백엔드 연결 오류 급증: {{ $labels.proxy }}"
          description:
            "백엔드 {{ $labels.proxy }} 연결 오류가 초당 100회를 초과했습니다."

      # [What] HAProxy High HTTP 4xx Error Rate / HAProxy 4xx 에러율 높음
      # [Why] Too many 4xx errors on backend. / 백엔드에서 4xx 에러가 너무 많습니다.
      - alert: HaproxyHighHttp4xxErrorRateBackend
        expr:
          ((sum by (proxy)
          (rate(haproxy_server_http_responses_total{code="4xx"}[1m])) / sum by
          (proxy) (rate(haproxy_server_http_responses_total[1m]))) * 100) > 5
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "HAProxy 4xx 에러율 높음: {{ $labels.proxy }}"
          description:
            "백엔드 {{ $labels.proxy }}의 4xx 에러율이 5%를 초과했습니다."

      # [What] HAProxy High HTTP 5xx Error Rate / HAProxy 5xx 에러율 높음
      # [Why] Too many 5xx errors on backend. / 백엔드에서 5xx 에러가 너무 많습니다.
      - alert: HaproxyHighHttp5xxErrorRateBackend
        expr:
          ((sum by (proxy)
          (rate(haproxy_server_http_responses_total{code="5xx"}[1m])) / sum by
          (proxy) (rate(haproxy_server_http_responses_total[1m]))) * 100) > 5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "HAProxy 5xx 에러율 높음: {{ $labels.proxy }}"
          description:
            "백엔드 {{ $labels.proxy }}의 5xx 에러율이 5%를 초과했습니다."

      # [What] HAProxy Backend Down / HAProxy 백엔드 다운
      # [Why] All servers in a backend are down. / 백엔드의 모든 서버가 다운되었습니다.
      - alert: HaproxyBackendDown
        expr: haproxy_backend_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "HAProxy 백엔드 다운: {{ $labels.proxy }}"
          description:
            "HAProxy 백엔드 {{ $labels.proxy }}의 모든 서버가 다운되었습니다."

      # [What] Traefik Down / Traefik 다운
      # [Why] Main ingress failure. / 메인 인그레스 실패.
      - alert: TraefikDown
        expr: up{job="traefik"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Traefik 다운"
          description: "Traefik 로드밸런서가 응답하지 않습니다."

      # [What] Traefik Service Down / Traefik 서비스 다운
      # [Why] All backend servers for a service are down. / 서비스의 모든 백엔드 서버가 다운되었습니다.
      - alert: TraefikServiceDown
        expr: count(traefik_service_server_up) by (service) == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Traefik 서비스 다운: {{ $labels.service }}"
          description:
            "Traefik 서비스 {{ $labels.service }}의 모든 백엔드가
            다운되었습니다."

      # [What] Traefik High HTTP 4xx Error Rate / Traefik 4xx 에러율 높음
      # [Why] Too many 4xx errors. / 4xx 에러가 너무 많습니다.
      - alert: TraefikHighHttp4xxErrorRateService
        expr:
          sum(rate(traefik_service_requests_total{code=~"4.*"}[3m])) by
          (service) / sum(rate(traefik_service_requests_total[3m])) by (service)
          * 100 > 5
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Traefik 4xx 에러율 높음: {{ $labels.service }}"
          description:
            "Traefik 서비스 {{ $labels.service }}의 4xx 에러율이 5%를
            초과했습니다."

      # [What] Traefik High HTTP 5xx Error Rate / Traefik 5xx 에러율 높음
      # [Why] Too many 5xx errors. / 5xx 에러가 너무 많습니다.
      - alert: TraefikHighHttp5xxErrorRateService
        expr:
          sum(rate(traefik_service_requests_total{code=~"5.*"}[3m])) by
          (service) / sum(rate(traefik_service_requests_total[3m])) by (service)
          * 100 > 5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Traefik 5xx 에러율 높음: {{ $labels.service }}"
          description:
            "Traefik 서비스 {{ $labels.service }}의 5xx 에러율이 5%를
            초과했습니다."

      # [What] Nginx High Error Rate / Nginx 에러율 높음
      # [Why] Web server failing requests. / 웹 서버 요청 실패율 증가.
      - alert: NginxHighHttp5xxErrorRate
        expr:
          sum(rate(nginx_http_requests_total{status=~"^5.."}[1m])) /
          sum(rate(nginx_http_requests_total[1m])) * 100 > 5
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Nginx 5xx 에러율 5% 초과"
          description: "Nginx 서버에서 5xx 에러가 5% 이상 발생하고 있습니다."

  # ============================================================================
  # [11] Observability Stack (LGTM)
  # ============================================================================
  - name: lgtm_alerts
    rules:
      # [What] Loki Request Errors / Loki 요청 에러
      # [Why] Log ingestion failing. / 로그 수집 실패.
      - alert: LokiRequestErrors
        expr:
          100 *
          sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m]))
          by (namespace, job, route) /
          sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job,
          route) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Loki 요청 에러율 10% 초과"
          description:
            "Loki 경로 {{ $labels.route }} 에서 에러가 10% 이상 발생 중입니다."

      # [What] Loki Process Too Many Restarts / Loki 과다 재시작
      # [Why] Loki has restarted too many times. / Loki가 너무 자주 재시작되었습니다.
      - alert: LokiProcessTooManyRestarts
        expr: changes(process_start_time_seconds{job=~".*loki.*"}[15m]) > 2
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Loki 과다 재시작"
          description: "Loki가 15분 내 2회 이상 재시작되었습니다."

      # [What] Loki Request Panic / Loki 패닉 발생
      # [Why] Loki is experiencing panics. / Loki에서 패닉이 발생하고 있습니다.
      - alert: LokiRequestPanic
        expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Loki 패닉 발생"
          description: "Loki에서 패닉이 발생했습니다."

      # [What] Loki Request Latency High / Loki 요청 지연 높음
      # [Why] Loki request latency is too high. / Loki 요청 지연이 너무 높습니다.
      - alert: LokiRequestLatency
        expr:
          (histogram_quantile(0.99,
          sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m]))
          by (le))) > 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Loki 요청 지연 높음"
          description: "Loki 99 퍼센타일 요청 지연이 1초를 초과했습니다."

      # [What] Alloy Down / Alloy 다운
      # [Why] Telemetry collection stopped. / 텔레메트리 수집 중단.
      - alert: GrafanaAlloyDown
        expr: up{job="alloy"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Grafana Alloy 다운"
          description: "Alloy 메트릭 수집기가 응답하지 않습니다."

      # [What] Tempo Down / Tempo 다운
      # [Why] Tempo tracing service is down. / Tempo 트레이싱 서비스가 다운되었습니다.
      - alert: TempoDown
        expr: up{job="tempo"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Tempo 다운"
          description: "Tempo 트레이싱 서비스가 응답하지 않습니다."

  # ============================================================================
  # [12] Authentication (Keycloak)
  # ============================================================================
  - name: keycloak_alerts
    rules:
      # [What] Keycloak Down / Keycloak 다운
      # [Why] Authentication service unavailable. / 인증 서비스 접속 불가.
      - alert: KeycloakDown
        expr: up{job="keycloak"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Keycloak 다운"
          description: "Keycloak 인증 서버가 응답하지 않습니다."

      # [What] Keycloak High Login Failures / Keycloak 로그인 실패 높음
      # [Why] Too many failed login attempts. / 로그인 실패 시도가 너무 많습니다.
      - alert: KeycloakHighLoginFailures
        expr: increase(keycloak_login_error_total[5m]) > 10
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Keycloak 로그인 실패 급증"
          description: "5분 내 로그인 실패가 10회를 초과했습니다."

      # [What] Keycloak Token Refresh Failed / Keycloak 토큰 갱신 실패
      # [Why] Token refresh operations are failing. / 토큰 갱신 작업이 실패하고 있습니다.
      - alert: KeycloakTokenRefreshFailed
        expr: increase(keycloak_refresh_token_error_total[5m]) > 10
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Keycloak 토큰 갱신 실패 급증"
          description: "5분 내 토큰 갱신 실패가 10회를 초과했습니다."
