# Hy-Home System Optimization Headers (Standardized)
x-security-baseline: &security-baseline
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL

x-logging: &default-logging
  driver: 'json-file'
  options:
    max-size: '5m'
    max-file: '2'

x-resource-low: &resource-low
  deploy:
    resources:
      limits: { cpus: '0.2', memory: 128M }
      reservations: { memory: 64M }

x-resource-med: &resource-med
  deploy:
    resources:
      limits: { cpus: '0.5', memory: 512M }
      reservations: { memory: 256M }

x-resource-high: &resource-high
  deploy:
    resources:
      limits: { cpus: '1.0', memory: 1G }
      reservations: { memory: 512M }

x-restart: &default-restart
  restart: unless-stopped

name: airflow

x-airflow-common: &airflow-common
  <<: [*default-restart, *security-baseline, *resource-med]
  profiles:
    - airflow
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:slim-3.1.6}
  networks:
    - infra_net
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_CMD: echo "postgresql+psycopg2://${AIRFLOW_DB_USER}:$$(cat /run/secrets/airflow_db_password)@${POSTGRES_MNG_HOSTNAME}:${POSTGRES_PORT:-5432}/airflow"
    AIRFLOW__CELERY__RESULT_BACKEND_CMD: echo "db+postgresql://${AIRFLOW_DB_USER}:$$(cat /run/secrets/airflow_db_password)@${POSTGRES_MNG_HOSTNAME}:${POSTGRES_PORT:-5432}/airflow"
    AIRFLOW__CELERY__BROKER_URL_CMD: echo "redis://:$$(cat /run/secrets/airflow_valkey_password)@${VALKEY_MNG_HOSTNAME}:${VALKEY_PORT:-6379}/0"
    AIRFLOW__CORE__FERNET_KEY_CMD: cat /run/secrets/airflow_fernet_key
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
    AIRFLOW__WEBSERVER__BASE_URL: 'https://airflow.${DEFAULT_URL}'
    AIRFLOW__METRICS__STATSD_ON: 'true'
    AIRFLOW__METRICS__STATSD_HOST: 'airflow-statsd-exporter'
    AIRFLOW__METRICS__STATSD_PORT: '${STATSD_AIRFLOW_PORT:-9125}'
    AIRFLOW__METRICS__STATSD_PREFIX: 'airflow'
  volumes:
    - airflow-dags:/opt/airflow/dags
    - airflow-plugins:/opt/airflow/plugins
    - airflow-logs:/opt/airflow/logs
    - airflow-config:/opt/airflow/config
  user: '${AIRFLOW_UID:-50000}:0'
  secrets:
    - airflow_db_password
    - airflow_fernet_key
    - airflow_www_password
    - airflow_valkey_password
  logging: *default-logging

volumes:
  airflow-dags:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ${DEFAULT_WORKFLOW_DIR}/airflow/dags
  airflow-logs:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ${DEFAULT_WORKFLOW_DIR}/airflow/logs
  airflow-config:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ${DEFAULT_WORKFLOW_DIR}/airflow/config
  airflow-plugins:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ${DEFAULT_WORKFLOW_DIR}/airflow/plugins
  airflow-valkey-data:

services:
  airflow-apiserver:
    <<: *airflow-common
    command: api-server
    healthcheck:
      test: ['CMD', 'curl', '--fail', 'http://localhost:${AIRFLOW_PORT:-8080}/api/v2/version']
      interval: 15s
      timeout: 30s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    labels:
      - 'traefik.enable=true'
      - 'traefik.http.routers.airflow.rule=Host(`airflow.${DEFAULT_URL}`)'
      - 'traefik.http.routers.airflow.entrypoints=websecure'
      - 'traefik.http.routers.airflow.tls=true'
      - 'traefik.http.services.airflow.loadbalancer.server.port=${AIRFLOW_PORT:-8080}'

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ['CMD', 'curl', '--fail', 'http://localhost:8974/health']
      interval: 15s
      timeout: 30s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test: ['CMD-SHELL', 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 15s
      timeout: 30s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - 'CMD-SHELL'
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 15s
      timeout: 30s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: '0'
    restart: unless-stopped
    depends_on:
      airflow-apiserver:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ['CMD-SHELL', 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 15s
      timeout: 30s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        /entrypoint airflow version
        /entrypoint airflow config list >/dev/null
        chown -R "${AIRFLOW_UID:-50000}:0" /opt/airflow/
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD_CMD: cat /run/secrets/airflow_www_password
    user: '0:0'

  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    healthcheck:
      test: ['CMD', 'curl', '--fail', 'http://localhost:${FLOWER_PORT:-5555}/']
      interval: 15s
      timeout: 30s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    labels:
      - 'traefik.enable=true'
      - 'traefik.http.routers.flower.rule=Host(`flower.${DEFAULT_URL}`)'
      - 'traefik.http.routers.flower.entrypoints=websecure'
      - 'traefik.http.routers.flower.tls=true'
      - 'traefik.http.services.flower.loadbalancer.server.port=${FLOWER_PORT:-5555}'
      - 'traefik.http.routers.flower.middlewares=sso-auth@file'

  airflow-valkey:
    <<: [*default-restart, *security-baseline]
    deploy:
      resources:
        limits: { cpus: '2.0', memory: 1G }
    image: valkey/valkey:9.0.2-alpine
    container_name: airflow-valkey
    secrets:
      - airflow_valkey_password
    networks:
      infra_net:
        ipv4_address: 172.19.0.15
    restart: unless-stopped
    volumes:
      - airflow-valkey-data:/data
    command: >
      sh -c '
      valkey-server
      --requirepass "$$(cat /run/secrets/airflow_valkey_password)"
      --appendonly yes
      --port 6379
      '
    expose:
      - '${VALKEY_PORT:-6379}'
    healthcheck:
      test:
        [
          'CMD-SHELL',
          'V_PASS=$$(cat /run/secrets/airflow_valkey_password); valkey-cli -a $$V_PASS -h 127.0.0.1 -p 6379 ping | grep PONG',
        ]
      interval: 15s
      timeout: 30s
      retries: 5
    logging: *default-logging

  airflow-statsd-exporter:
    profiles:
      - airflow
    image: prom/statsd-exporter:v0.28.0

    container_name: airflow-statsd-exporter
    restart: unless-stopped
    command: '--statsd.listen-udp=:${STATSD_AIRFLOW_PORT:-9125} --web.listen-address=:${STATSD_PROMETHEUS_PORT:-9102} --statsd.mapping-config=/tmp/mappings.yml'
    volumes:
      - ./config/statsd_mapping.yml:/tmp/mappings.yml:ro
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - infra_net
    logging: *default-logging

secrets:
  airflow_db_password:
    external: true
  airflow_fernet_key:
    external: true
  airflow_www_password:
    external: true
  airflow_valkey_password:
    external: true

networks:
  infra_net:
    name: infra_net
    external: true
