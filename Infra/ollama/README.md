# Ollama (ë¡œì»¬ LLM í”Œë«í¼)

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì—ì„œì˜ ì—­í• 

OllamaëŠ” **ë¡œì»¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ì‹¤í–‰ í”Œë«í¼**ìœ¼ë¡œì„œ On-Premise í™˜ê²½ì—ì„œ AI ëª¨ë¸ì„ êµ¬ë™í•©ë‹ˆë‹¤. Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ Open WebUIë¥¼ í†µí•©í•˜ì—¬ RAG(Retrieval-Augmented Generation) ê¸°ë°˜ ì±—ë´‡ ë° AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

**í•µì‹¬ ì—­í• :**

- ğŸ¤– **ë¡œì»¬ LLM ì‹¤í–‰**: GPU ê¸°ë°˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í˜¸ìŠ¤íŒ…
- ğŸ” **RAG ì‹œìŠ¤í…œ**: ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ë¬¸ì„œ ì°¸ì¡°í˜• ì‘ë‹µ
- ğŸ’¬ **ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤**: ChatGPT ìŠ¤íƒ€ì¼ ì›¹ UI
- ğŸ“š **ì„ë² ë”© ìƒì„±**: í…ìŠ¤íŠ¸ ë²¡í„°í™” ë° ì˜ë¯¸ ê²€ìƒ‰
- ğŸ” **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ**: ì™¸ë¶€ API ì˜ì¡´ ì—†ëŠ” ì™„ì „í•œ ë¡œì»¬ ì²˜ë¦¬

## ì•„í‚¤í…ì²˜ êµ¬ì„±

```mermaid
flowchart TB
    subgraph "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤"
        USER[ì‚¬ìš©ì]
        WEBUI[Open WebUI<br/>Chat Interface]
    end
    
    subgraph "LLM ì—”ì§„"
        OLLAMA[Ollama<br/>Model Server]
    end
    
    subgraph "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤"
        QDRANT[Qdrant<br/>Vector DB]
    end
    
    subgraph "ëª¨ë‹ˆí„°ë§"
        OEXP[ollama-exporter]
        PROM[Prometheus]
    end
    
    subgraph "AI ëª¨ë¸"
        LLM[LLM Models<br/>llama3, qwen ë“±]
        EMB[Embedding Model<br/>qwen3-embedding]
    end
    
    USER -->|Chat| WEBUI
    
    WEBUI -->|Query| OLLAMA
    WEBUI -->|Store Docs| QDRANT
    WEBUI -->|Vector Search| QDRANT
    
    OLLAMA -->|Load| LLM
    OLLAMA -->|Embedding| EMB
    
    EMB -->|Vectors| QDRANT
    
    OLLAMA -->|ë©”íŠ¸ë¦­| OEXP
    OEXP -->|ìˆ˜ì§‘| PROM
```

## ì£¼ìš” êµ¬ì„± ìš”ì†Œ

### 1. Ollama (LLM ì„œë²„)

- **ì»¨í…Œì´ë„ˆ**: `ollama`
- **ì´ë¯¸ì§€**: `ollama/ollama:0.13.1`
- **ì—­í• **: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì‹¤í–‰ ì—”ì§„
- **í¬íŠ¸**: `${OLLAMA_PORT}` (ê¸°ë³¸ 11434)
- **Traefik**: `https://ollama.${DEFAULT_URL}`
- **IP**: 172.19.0.40

**GPU ì„¤ì •:**

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

**í™˜ê²½ ë³€ìˆ˜:**

- `OLLAMA_HOST=0.0.0.0:${OLLAMA_PORT}`: ëª¨ë“  ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì‹ 
- `NVIDIA_VISIBLE_DEVICES=all`: ëª¨ë“  GPU ì‚¬ìš©

**ë³¼ë¥¨:**

- `ollama-data:/root/.ollama`: ëª¨ë¸ ì €ì¥ì†Œ

### 2. Qdrant (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤)

- **ì»¨í…Œì´ë„ˆ**: `qdrant`
- **ì´ë¯¸ì§€**: `qdrant/qdrant:latest`
- **ì—­í• **: ë²¡í„° ì„ë² ë”© ì €ì¥ ë° ì˜ë¯¸ ê²€ìƒ‰
- **í¬íŠ¸**: `${QDRANT_HOST_PORT}:${QDRANT_PORT}` (ê¸°ë³¸ 6333)
- **Traefik**: `https://qdrant.${DEFAULT_URL}`
- **IP**: 172.19.0.41

**ì£¼ìš” ê¸°ëŠ¥:**

- ê³ ì„±ëŠ¥ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
- ì›¹ UI ëŒ€ì‹œë³´ë“œ
- REST API ë° gRPC
- ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (`QDRANT__TELEMETRY_DISABLED=false`)

**ë³¼ë¥¨:**

- `qdrant-data:/qdrant/storage`

### 3. Open WebUI (ì±—ë´‡ UI)

- **ì»¨í…Œì´ë„ˆ**: `open-webui`
- **ì´ë¯¸ì§€**: `ghcr.io/open-webui/open-webui:main`
- **ì—­í• **: ChatGPT ìŠ¤íƒ€ì¼ ì›¹ ì¸í„°í˜ì´ìŠ¤
- **í¬íŠ¸**: 8080 (ë‚´ë¶€)
- **Traefik**: `https://chat.${DEFAULT_URL}`
- **IP**: 172.19.0.42

**Ollama ì—°ê²°:**

- `OLLAMA_BASE_URL=http://ollama:${OLLAMA_PORT}`

**RAG ì„¤ì •:**

- `VECTOR_DB_URL=http://qdrant:${QDRANT_PORT}`
- `RAG_EMBEDDING_ENGINE=ollama`
- `RAG_EMBEDDING_MODEL=qwen3-embedding:0.6b`

**ë³¼ë¥¨:**

- `ollama-webui:/app/backend/data`: ì±„íŒ… ê¸°ë¡, ì‚¬ìš©ì ì„¤ì •

### 4. Ollama Exporter (ëª¨ë‹ˆí„°ë§)

- **ì»¨í…Œì´ë„ˆ**: `ollama-exporter`
- **ì´ë¯¸ì§€**: `lucabecker42/ollama-exporter:latest`
- **ì—­í• **: Ollama ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **í¬íŠ¸**: `${OLLAMA_EXPORTER_HOST_PORT}:${OLLAMA_EXPORTER_PORT}` (ê¸°ë³¸ 9090)
- **IP**: 172.19.0.43

**ë©”íŠ¸ë¦­:**

- ëª¨ë¸ ë¡œë“œ ìƒíƒœ
- ì¶”ë¡  ìš”ì²­ ìˆ˜
- GPU ì‚¬ìš©ë¥ 

## í™˜ê²½ ë³€ìˆ˜

### .env íŒŒì¼

```bash
# Ollama
OLLAMA_PORT=11434
OLLAMA_HOST_PORT=11434

# Qdrant
QDRANT_PORT=6333
QDRANT_HOST_PORT=6333

# Open WebUI
OLLAMA_WEBUI_PORT=8080
OLLAMA_WEBUI_HOST_PORT=3000

# Exporter
OLLAMA_EXPORTER_PORT=9090
OLLAMA_EXPORTER_HOST_PORT=9090

# ë„ë©”ì¸
DEFAULT_URL=127.0.0.1.nip.io
```

## ë„¤íŠ¸ì›Œí¬

- **ë„¤íŠ¸ì›Œí¬**: `infra_net`
- **ì„œë¸Œë„·**: 172.19.0.0/16
- **ê³ ì • IP**: 172.19.0.40-43

## ì‹œì‘ ë°©ë²•

### 1. GPU ë“œë¼ì´ë²„ í™•ì¸ (í•„ìˆ˜)

```bash
# NVIDIA GPU í™•ì¸
nvidia-smi

# Docker GPU ëŸ°íƒ€ì„ í™•ì¸
docker run --rm --gpus all ubuntu nvidia-smi
```

### 2. ì„œë¹„ìŠ¤ ì‹œì‘

```bash
cd d:\hy-home.docker\Infra\ollama
docker-compose up -d
```

### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# Ollama ì»¨í…Œì´ë„ˆì—ì„œ ëª¨ë¸ pull
docker exec -it ollama ollama pull llama3.2:3b
docker exec -it ollama ollama pull qwen3-embedding:0.6b

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
# llama3, qwen, mistral, gemma, phi ë“±
# https://ollama.com/library
```

### 4. ëª¨ë¸ í™•ì¸

```bash
# ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
docker exec ollama ollama list
```

## ì ‘ì† ì •ë³´

### Open WebUI (Chat Interface)

- **URL**: `https://chat.127.0.0.1.nip.io`
- **ì´ˆê¸° ì ‘ì†**: ì‚¬ìš©ì ê³„ì • ìƒì„± í•„ìš”
- **ê¸°ëŠ¥**: ì±„íŒ…, ë¬¸ì„œ ì—…ë¡œë“œ, RAG ê²€ìƒ‰

### Qdrant Dashboard

- **URL**: `https://qdrant.${DEFAULT_URL}` or `http://localhost:6333/dashboard`
- **ê¸°ëŠ¥**: ì»¬ë ‰ì…˜ ê´€ë¦¬, ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸

### Ollama API

- **URL**: `https://ollama.${DEFAULT_URL}`
- **Endpoint**: `/api/generate`, `/api/chat`, `/api/embeddings`

## ìœ ìš©í•œ ëª…ë ¹ì–´

### ëª¨ë¸ ê´€ë¦¬

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec ollama ollama pull llama3.2:latest

# ëª¨ë¸ ëª©ë¡
docker exec ollama ollama list

# ëª¨ë¸ ì‚­ì œ
docker exec ollama ollama rm llama3.2:3b

# ëª¨ë¸ ì •ë³´
docker exec ollama ollama show llama3.2:3b
```

### CLI í…ŒìŠ¤íŠ¸

```bash
# ëŒ€í™”í˜• ëª¨ë“œ
docker exec -it ollama ollama run llama3.2:3b

# ë‹¨ì¼ ì§ˆë¬¸
docker exec ollama ollama run llama3.2:3b "What is Docker?"
```

### API í…ŒìŠ¤íŠ¸

```bash
# Generate ì—”ë“œí¬ì¸íŠ¸
curl https://ollama.127.0.0.1.nip.io/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "Why is the sky blue?",
  "stream": false
}'

# ì„ë² ë”© ìƒì„±
curl https://ollama.127.0.0.1.nip.io/api/embeddings -d '{
  "model": "qwen3-embedding:0.6b",
  "prompt": "Hello World"
}'
```

### Qdrant ê´€ë¦¬

```bash
# ì»¬ë ‰ì…˜ ìƒì„±
curl -X PUT https://qdrant.127.0.0.1.nip.io/collections/documents \
  -H "Content-Type: application/json" \
  -d '{
    "vectors": {
      "size": 384,
      "distance": "Cosine"
    }
  }'

# ë²¡í„° ì‚½ì…
curl -X PUT https://qdrant.127.0.0.1.nip.io/collections/documents/points \
  -H "Content-Type: application/json" \
  -d '{
    "points": [
      {
        "id": 1,
        "vector": [0.05, 0.61, ...],
        "payload": {"text": "Sample document"}
      }
    ]
  }'

# ìœ ì‚¬ë„ ê²€ìƒ‰
curl -X POST https://qdrant.127.0.0.1.nip.io/collections/documents/points/search \
  -H "Content-Type: application/json" \
  -d '{"vector": [0.05, 0.61, ...], "limit": 5}'
```

## ë°ì´í„° ì˜ì†ì„±

### ë³¼ë¥¨

- `ollama-data`: Ollama ëª¨ë¸ ë° ì„¤ì • (`/root/.ollama`)
- `qdrant-data`: ë²¡í„° ë°ì´í„° (`/qdrant/storage`)
- `ollama-webui`: ì±„íŒ… ê¸°ë¡ ë° ì—…ë¡œë“œ íŒŒì¼ (`/app/backend/data`)

### ëª¨ë¸ ì €ì¥ ìœ„ì¹˜

```bash
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€
/root/.ollama/models/

# í˜¸ìŠ¤íŠ¸ ë³¼ë¥¨
docker volume inspect ollama-data
```

## RAG ì›Œí¬í”Œë¡œìš°

### 1. ë¬¸ì„œ ì—…ë¡œë“œ

```
ì‚¬ìš©ì â†’ Open WebUI â†’ ë¬¸ì„œ ì—…ë¡œë“œ â†’ Ollama Embedding â†’ Qdrant ì €ì¥
```

### 2. ì§ˆë¬¸ ì‘ë‹µ

```
ì‚¬ìš©ì ì§ˆë¬¸ â†’ Embedding â†’ Qdrant ê²€ìƒ‰ â†’ ê´€ë ¨ ë¬¸ì„œ ì¡°íšŒ â†’ LLM ì¶”ë¡  â†’ ì‘ë‹µ
```

### 3. ì§€ì› íŒŒì¼ í˜•ì‹

- PDF, DOC, DOCX
- TXT, MD
- CSV, JSON
- Images (OCR)

## ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

### GPU ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
# nvidia-smi (í˜¸ìŠ¤íŠ¸)
nvidia-smi

# ì»¨í…Œì´ë„ˆ GPU ì‚¬ìš©ëŸ‰
docker stats ollama
```

### ì„±ëŠ¥ íŠœë‹

```bash
# ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
OLLAMA_MAX_LOADED_MODELS=1

# ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
OLLAMA_NUM_CTX=2048

# GPU ë ˆì´ì–´ ìˆ˜
OLLAMA_GPU_LAYERS=32
```

## ë¬¸ì œ í•´ê²°

### GPU ì¸ì‹ ì•ˆë¨

```bash
# GPU í• ë‹¹ í™•ì¸
docker exec ollama nvidia-smi

# Docker GPU ëŸ°íƒ€ì„ ì¬ì„¤ì •
sudo systemctl restart docker
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼

```bash
# í”„ë¡ì‹œ ì„¤ì •
HTTPS_PROXY=http://proxy:port docker exec ollama ollama pull llama3.2:3b
```

### Out of Memory

```bash
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull llama3.2:1b  # ëŒ€ì‹  llama3.2:3b

# ëª¨ë¸ ì–¸ë¡œë“œ
ollama stop llama3.2:8b
```

## ì‹œìŠ¤í…œ í†µí•©

### ì˜ì¡´í•˜ëŠ” ì„œë¹„ìŠ¤

- **NVIDIA GPU ë“œë¼ì´ë²„**: í•„ìˆ˜
- **Docker GPU Runtime**: í•„ìˆ˜
- **Traefik**: HTTPS ë¼ìš°íŒ…

### ì´ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜

- **ì±—ë´‡**: ê³ ê° ì§€ì›, FAQ
- **ë¬¸ì„œ ê²€ìƒ‰**: ì‚¬ë‚´ ì§€ì‹ ë² ì´ìŠ¤
- **ì½”ë“œ ìƒì„±**: AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸
- **ë°ì´í„° ë¶„ì„**: ìì—°ì–´ ì¿¼ë¦¬

## ì¶”ì²œ ëª¨ë¸

### ëŒ€í™”í˜•

- `llama3.2:3b` - ë¹ ë¥¸ ì‘ë‹µ, ì¤‘ê°„ í’ˆì§ˆ
- `qwen2.5:14b` - ê³ í’ˆì§ˆ í•œêµ­ì–´ ì§€ì›
- `gemma2:9b` - Google ëª¨ë¸, ìš°ìˆ˜í•œ ì„±ëŠ¥

### ì„ë² ë”©

- `qwen3-embedding:0.6b` - ë¹ ë¥´ê³  ê°€ë²¼ì›€
- `nomic-embed-text` - ì˜ì–´ ìµœì í™”

### ì½”ë“œ ìƒì„±

- `codellama:7b` - ì½”ë“œ ìƒì„± ì „ë¬¸
- `qwen2.5-coder:7b` - ë‹¤êµ­ì–´ ì½”ë“œ ì§€ì›

## ì°¸ê³  ìë£Œ

- [Ollama ê³µì‹ ì‚¬ì´íŠ¸](https://ollama.com/)
- [Ollama ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬](https://ollama.com/library)
- [Qdrant ë¬¸ì„œ](https://qdrant.tech/documentation/)
- [Open WebUI GitHub](https://github.com/open-webui/open-webui)
- [RAG ê°€ì´ë“œ](https://docs.llamaindex.ai/en/stable/)
